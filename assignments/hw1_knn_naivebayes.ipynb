{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aX3fNdxFnlH"
   },
   "source": [
    "# Homework 1 - KNN and Naive Bayes Classification\n",
    "\n",
    "## Problem 1 - EMNIST Letters Classification\n",
    "*This problem is based on the MNIST Exercises from Chapter 2 of Applied Machine Learning by David Forsyth (2019).*\n",
    "\n",
    "[EMNIST [1]](https://www.nist.gov/itl/products-and-services/emnist-dataset) is a large image dataset for character and letter recognition.  It was designed to be an extended version created in the same format as the original (and very popular) MNIST dataset which contained only digits.\n",
    "\n",
    "EMNIST contains a variety of sub datasets for general character, digit only, and alphabetical character only recognition.  In this problem, we are going to look at EMNIST Letters:  a balanced dataset with only alphabetical letters (26 classes, upper and lowercase are both labeled to the same class).\n",
    "\n",
    "1. Load in the dataset (both training and test sets) so that each sample (e.g. image) is one row.  Each row should be of lenght 784 (each image is $28 \\times 28$, so if you view each pixel as a feature you would have $28 \\times 28$ features for each image.  You can confirm that you've done it correctly by looking at the 20th image in the training set (which should very clearly be a `B`).  You can plot it with the following where you would need to uncomment one of the two `order = ...` lines (which one depends on which of the two options below that you use to access the data.)\n",
    "```\n",
    "#order = 'c' \n",
    "#order = 'f'\n",
    "plt.imshow(X_train[20,:].reshape((28,28),order=order), cmap=\"Greys\")\n",
    "print(y_train[20])\n",
    "```\n",
    "You can access the data by either of the following (which one you choose may impact a little preprocessing you have to do to get everything to the correct shape.)\n",
    "* Option 1:  using the [emnist python package](https://pypi.org/project/emnist/) to access the dataset\n",
    "* Option 2:  downloading the dataset from EMNIST's website (if you do this I'd recommend the .mat version and looking at resources on how to load the mat in Python, such as [https://stackoverflow.com/questions/51125969/loading-emnist-letters-dataset](https://stackoverflow.com/questions/51125969/loading-emnist-letters-dataset))\n",
    "\n",
    "2. Using sklearn, build and evaluate a k-nearest neighbors classifier.  Use cross-validation to evaluate (you might want to look at [sklearn's docs on how to use cross-validation with their models](https://scikit-learn.org/stable/modules/cross_validation.html), what validation accuracy do you get? Choose a relatively small value for $k$ (somewhere between 3 and 5).\n",
    "\n",
    "3. Build and evaluate an approximate nearest neighbors classifier using FLANN.  [FLANN [2]](https://github.com/flann-lib/flann) is a package for performing nearest neighbor search in high-dimensional spaces.  It's actually in C++, but there's python bindings available in `pyflann`.  Admittedly, the pyflann documentation isn't great, so you'll probably need to do some googling to figure out the calls.  You want to use `build_index` and `nn_index` rather than just `nn` which is what they show in most examples (`nn` isn't fantastic because it rebuilds the index every time).  You can either implement cross-validation on your own or you can just separate the training set into a training set and validation set (since there's no easy way to do cross-validation automatically with flann).  Note -- when you use pyflann to predict, it doesn't \n",
    "\n",
    "* How does the validation accuracy compare to sklearn's nearest neighbors?\n",
    "* How about the runtime? (you don't have to be super precise here -- it should tell you how long it took to run the code cell which should give you a high-level comparison).\n",
    "* Try different number of neighbors between 2 and 10.  Based on the validation accuracy, what is the best value for number of neighbors? \n",
    "\n",
    "4. Using sklearn, build and evaluate a Gaussian Naive Bayes classifier.  How well does it do?  For comparison, try building a Gaussian Naive Bayes Classifier on the EMNIST digits dataset -- does Gaussian Naive Bayes do better on the digits than it does on the letters (pay attention to the baseline accuracy for each).\n",
    "\n",
    "5. Can you improve one or more of these models (this will take experimentation).  Things to try would include preprocessing of the data (thresholding, re-centering/stretching with different bounding boxes, data augmentation, etc.).  It's okay if you can't improve, just try a couple things).  \n",
    "\n",
    "5. Based on the validation accuracies you achieved in the previous parts, what is the best model?  Evaluate that model on the test set.  Create a confusion matrix to evaluate the results -- which letters is it most unable to determine between? Since there are so many classes -- you should probably visualize the conusion matrix with a plot (see [sklearn's ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)\n",
    "\n",
    "6. **Extra Credit (optional)**:  Using another sub dataset from the EMNIST dataset (other than the MNIST subset and the letters subset that you looked at in 1-5), experiment (using a validation set or cross-validation) with a variety of K-nearest neighbors (or approximate nearest neighbors) and Naive Bayes and experiment with a variety of data preprocessing to try to identify the best model.  Demonstrate the overall effectiveness of the best model by evaluating the accuracy on the test set and displaying a confusion matrix.\n",
    "\n",
    "[1] Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters. Retrieved from http://arxiv.org/abs/1702.05373\n",
    "\n",
    "[2] Marius Muja and David G. Lowe, \"Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration\", in International Conference on Computer Vision Theory and Applications (VISAPP'09), 2009, https://www.cs.ubc.ca/~lowe/papers/09muja.pdf\n",
    "\n",
    "\n",
    "## Problem 2 - SMS Spam vs Ham Classification\n",
    "A common use of Naive Bayes is for text classification.  When using Naive Bayes for text classification a common choice is to use a \"bag of words\" representation of a document.  The \"bag of words\" representation assumes position doesn't matter and instead focuses on how common each word is in a document.\n",
    "\n",
    "The dataset for this problem is [https://archive.ics.uci.edu/ml/datasets/sms+spam+collection](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection).  Don't forget to separate the dataset into train/test splits (this dataset isn't preseparated) and use cross-validation (either k-fold cross validation or by separating the dataset into train/test/validate subsets).  You might want to consider \"stratified\" sampling when doing your dataset splitting due to the imbalance in the dataset.\n",
    "\n",
    "1.  Using sklearn, build and evaluate a Multinomial Naive Bayes classifier to classify the text messages as \"spam\" or \"ham\" (e.g. not spam).  You'll need to get the word counts first (you may want to consider using [sklearn's CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), but you are also free to process the data manually).  \n",
    "2. Experiment with different preprocessing steps ( -- somethings you may want to consider are removing stop words, removing numbers, dropping infrequent words, etc.)\n",
    "3. Evaluate your best model on the test set.  How well did it do?  Calculate the test set accuracy. (How does this compare to the baseline accuracy?)  You can use your choice of other evaluation metrics and/or confusion matrix to present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQoVQM-VFOzX",
    "outputId": "5236f666-b67d-4935-cce8-b6d681062011"
   },
   "outputs": [],
   "source": [
    "!pip install pyflann-py3\n",
    "!pip install emnist"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
