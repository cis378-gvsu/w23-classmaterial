{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aX3fNdxFnlH"
      },
      "source": [
        "# Homework 3 - Decision Trees / Random Forests & Dimensionality Reduction\n",
        "\n",
        "## Problem 1 - EMNIST Letters Classification\n",
        "This problem is a continuation of homework 1 and 2, continuing to use\n",
        "[EMNIST [1]](https://www.nist.gov/itl/products-and-services/emnist-dataset).\n",
        "\n",
        "1. (Same as step 1 from HW1/HW2) Load in the dataset (both training and test sets) so that each sample (e.g. image) is one row.  Each row should be of lenght 784 (each image is $28 \\times 28$, so if you view each pixel as a feature you would have $28 \\times 28$ features for each image.  \n",
        "\n",
        "3. Using sklearn's DecisionTreeClassifier, build and evaluate a single decision tree.  Use either cross validation or a held out validation set to evaluate the performance of the model.  Look at the different parameters for a DecisionTreeClassifier model. Choose at least one, explain what it does in your own words, and then experiment with different values of that parameter to find a good value (you don't have to try all possible values, just experiment).  \n",
        "\n",
        "3. Using sklearn's RandomForestClassifier, build and evaluate a random forest (with bagging).  Use the \"out of bag\" samples to evaluate the performance (there's an option for this). Look at the different parameters for a RandomForestClassifier model. Choose at least one, explain what it does in your own words, and then experiment with different values of that parameter to find a good value (you don't have to try all possible values, just experiment).  \n",
        "\n",
        "4. Using sklearn's [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), build a boosted classifier.  Look at the different parameters for an AdaBoostClassifier model. Choose at least one, explain what it does in your own words, and then experiment with different values of that parameter to find a good value (you don't have to try all possible values, just experiment).  Use a held out validation set to evaluate the performance of the model.\n",
        "\n",
        "5. XGBoost is one of the most common boosted tree packages available.  It's written underneath in C++, but there are [bindings in many languages, including python](https://xgboost.readthedocs.io/en/stable/python/python_api.html).  They also have a [variety of examples for using it in Python](https://xgboost.readthedocs.io/en/stable/python/examples/index.html).  Using xgb's XGBClassifier, build and evaluate an XGBoost classification model.  Because there will be many decision trees built, I recommend using the following parameters to the model:\n",
        "   * `eval_metric='merror'` (this one is a must because we are doing multiclass classification).\n",
        "   * setting `n_jobs` to something between 4 and 8\n",
        "   * setting `max_depth` to something relatively small (e.g. something like 5, 8, 10, etc).\n",
        "\n",
        "   Also, if you are running on Colab I recommend:\n",
        "   * switching the runtime type to Hardware accelerator: \"GPU\" with Standard GPU (this is free).\n",
        "   * using the `tree-method=\"gpu-hist\"` as a parameter for the classifier (this will let it use the gpu).\n",
        "\n",
        "   Use a held out validation set to evaluate the performance of the model.\n",
        "\n",
        "6.  Using sklearn, perform PCA for dimensionality reduction.  You should try a variety of different dimensions and plot the explained variance percentage versus $k$.  This is an elbow plot -- look for where the elbow occurs to identify the best value of $k$.\n",
        "\n",
        "7. Using the best value of $k$, rerun the best model from your previous two assignments.  Does running PCA first improve the model's performance?\n",
        "\n",
        "8. Take your best performing model overall and evaluate it on the withheld test set.  Create a confusion matrix to display the results.  In words, how well does it do?  You should compare to baseline.  \n",
        "\n",
        "\n",
        "[1] Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters. Retrieved from http://arxiv.org/abs/1702.05373\n",
        "\n",
        "<br><br>\n",
        "## Problem 2 - Credit Card Fraud Classification\n",
        "For this proble, we will look at a dataset released by Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles).  Specifically, the dataset contains a large number of credit card transactions, classified as either fraudulent or non-fraudulent.  You can [download a version of the dataset on Google Drive](https://drive.google.com/file/d/1Lt3g6oXu0iqAenB6mZo5F5vwO3iem1fJ/view?usp=sharing). \n",
        "\n",
        "1. Read in the data.  The last column contains the class and the remaining columns contain features (the exact meaning of each feature is hidden to protect sensitive data).  Are the classes balanced in this dataset?  (you may want to keep this in mind for future steps).  You can use code similar to the following to read in the data if you are on Google Colab -- you will first need to upload a copy of the dataset to your Google Drive and replace the path with one that is correct for where you placed it.\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/CIS378/hw3/creditcard.csv')\n",
        "```\n",
        "\n",
        "2. Split your data into training and testing sets.  Make sure that the imbalanced will not potentially be worsened based on how you split.\n",
        "\n",
        "3. Try using one or more feature selection techniques.  This step can be done either before or after the next step, depending on which feature selection approach you choose.  Are there any features that you find are not particularly beneficial.  These could be filter techniques (done before training the model, wrapper methods involving retraining the model, embedded methods where feature selection is inherent to the model, or other techniques, such as permutation importance handled after the model is trained).\n",
        "\n",
        "4. Experiment with a variety of different classification techniques to identfiy the best model.  You should use either cross-validation, a separate validation set, or some other technique for measuring model performance (such as out-of-bag samples).  Explain briefly what models you tried, and and their performance.  Note, some models may be more naturally well-suited for handling imbalanced data.\n",
        "\n",
        "\n",
        "5. Evaluate your finally chosen model on the test set.  \n",
        "   * Create a confusion matrix with the results.\n",
        "   * How does its accuracy compare to the baseline accuracy for your test set? \n",
        "   * You may find it helfpul to use a [different sklearn metric](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) aside from just accuracy, due to the balance of classes in the dataset.\n",
        "   * Taking into account this specific application -- how good of a model is this?  What should be prioritzed?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rTEpoV-7wPLv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}